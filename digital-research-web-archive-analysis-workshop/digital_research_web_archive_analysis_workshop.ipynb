{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ruebot/notebooks/blob/main/digital-research-web-archive-analysis-workshop/digital_research_web_archive_analysis_workshop.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EN0gajhr7pKd"
      },
      "source": [
        "# Getting Started with Web Archive Analysis\n",
        "\n",
        "In this workshop we will download some example derivatives from [Archive-It's ARCH](https://webservices.archive.org/pages/arch) service to demonstrate a few examples of further exploration of web archive data. We'll be using derivatives created from the [Movimiento estudiantil feminista Universidad Autónoma del Estado de México 2020 ](https://archive-it.org/collections/20429) collection created by [Huellas Incómodas](https://archive-it.org/organizations/2521).\n",
        "\n",
        "## Archives Unleashed Toolkit Derivatives\n",
        "\n",
        "The web archive derivatives that we are working with, that ARCH created, are generated using the [Archives Unleashed Toolkit](https://github.com/archivesunleashed/aut). If you have W/ARC files of your own, you can create these same derivatives, and more! Examples of how to do this, and further documentation can be found [here](https://aut.docs.archivesunleashed.org/docs/auk-derivatives).\n",
        "\n",
        "## Notebook History\n",
        "\n",
        "This notebook is a derivative of:\n",
        "\n",
        "* [![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.6381036.svg)](https://doi.org/10.5281/zenodo.6381036) -- Working with ARCH Derivatives\n",
        "\n",
        "* Article: [Creating order from the mess: web archive derivative datasets and notebooks](https://doi.org/10.1080/23257962.2022.2100336)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OctPUqrG-K2W"
      },
      "source": [
        "# Datasets\n",
        "\n",
        "First, we will need to create variables for the derivative data from ARCH. For this, we'll just be using the download URLs for each of the derivatives. You can grab these by right-clicking on the download icon, and selecting \"Copy Link\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m3FRzjAYyPid"
      },
      "source": [
        "domain_frequency_data = \"https://webdata.archive-it.org/ait/files/download/ARCHIVEIT-ait:1796:20429/DomainFrequencyExtraction/domain-frequency.csv.gz?access=24VLVHYLY3J7FQTVLRPHUC6VC5PWU4KG\"\n",
        "image_info_data = \"https://webdata.archive-it.org/ait/files/download/ARCHIVEIT-ait:1796:20429/ImageInformationExtraction/image-information.csv.gz?access=Q5OVM44HHNOH5X2SN63AEQFTXPVGYUKY\"\n",
        "web_graph_data = \"https://webdata.archive-it.org/ait/files/download/ARCHIVEIT-ait:1796:20429/WebGraphExtraction/web-graph.csv.gz?access=5TAWDHCMR4I6XCQ2YGZO3AY4QOCXZCSS\"\n",
        "web_pages_data = \"https://webdata.archive-it.org/ait/files/download/ARCHIVEIT-ait:1796:20429/WebPagesExtraction/web-pages.csv.gz?access=3I7IQZFO5D4AJA3FURDJSRG4H7OZWGXW\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JS0T6i4xPTVx"
      },
      "source": [
        "# Environment\n",
        "\n",
        "Next, we'll setup our environment so we can load our derivatives into [pandas](https://pandas.pydata.org)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n56-DeEFPTCq"
      },
      "source": [
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JgBtx0xFFv_i"
      },
      "source": [
        "# Data Table Display\n",
        "\n",
        "Colab includes an extension that renders pandas dataframes into interactive displays that can be filtered, sorted, and explored dynamically. This can be very useful for taking a look at what each DataFrame provides!\n",
        "\n",
        "Data table display for pandas dataframes can be enabled by running:\n",
        "```python\n",
        "%load_ext google.colab.data_table\n",
        "```\n",
        "and disabled by running\n",
        "```python\n",
        "%unload_ext google.colab.data_table\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8h2lJY2Z7xyt"
      },
      "source": [
        "%load_ext google.colab.data_table"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oFAFb2X3_VJC"
      },
      "source": [
        "# Loading our ARCH Datasets as DataFrames\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "Next, we'll setup our datasets as pandas DataFrames to work with, and show a preview of each using the Data Table Display.\n",
        "\n",
        "Each block of derivative commands create a variable. That variable is a DataFrame with all of the information from a given derivative. After the DataFrame is created, a preview of it is shown."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ndZjsWKzUykd"
      },
      "source": [
        "### Domain Frequency\n",
        "\n",
        "Provides the following columns:\n",
        "\n",
        "* domain\n",
        "* count"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xUebRBMHuoRs"
      },
      "source": [
        "domain_frequency = pd.read_csv(domain_frequency_data, compression='gzip')\n",
        "domain_frequency"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ijcDwS7tdpoG"
      },
      "source": [
        "### Web Graph\n",
        "\n",
        "Provides the following columns:\n",
        "\n",
        "* crawl date\n",
        "* source\n",
        "* target\n",
        "* anchor text\n",
        "\n",
        "Note that this contains all links and is not aggregated into domains."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "web_graph = pd.read_csv(web_graph_data, compression='gzip')\n",
        "web_graph"
      ],
      "metadata": {
        "id": "XRwOGbstNKdC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SnEntLU0U2ox"
      },
      "source": [
        "### Images\n",
        "\n",
        "Provides the following columns:\n",
        "\n",
        "* crawl date\n",
        "* URL of the image\n",
        "* filename\n",
        "* image extension\n",
        "* MIME type as provided by the web server\n",
        "* MIME type as detected by Apache TIKA\n",
        "* image width\n",
        "* image height\n",
        "* image MD5 hash\n",
        "* image SHA1 hash"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GhE_Vhv8Awkx"
      },
      "source": [
        "images = pd.read_csv(image_info_data, compression='gzip')\n",
        "images"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_GTL6VtBydje"
      },
      "source": [
        "### Web Pages\n",
        "\n",
        "Provides the following columns:\n",
        "\n",
        "* crawl date\n",
        "* web domain\n",
        "* URL\n",
        "* MIME type as provided by the web server\n",
        "* MIME type as detected by Apache TIKA\n",
        "* content (HTTP headers and HTML removed)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jH5fqM9Tyfj7"
      },
      "source": [
        "web_pages = pd.read_csv(web_pages_data, compression='gzip')\n",
        "web_pages"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3HPwOCNAvqMe"
      },
      "source": [
        "# Data Analysis\n",
        "\n",
        "Now that we have all of our datasets loaded up, we can begin to work with them!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J6Pkg0prv3BE"
      },
      "source": [
        "## Counting total files, and unique files\n",
        "\n",
        "Let's take a quick look at how to count items in DataFrames, and use total and unique files as an example to work with.\n",
        "\n",
        "It's definitely work checking out the [pandas documentation](https://pandas.pydata.org/docs/index.html). There are a lot of good examples available, along with a robust [API reference](https://pandas.pydata.org/docs/reference/index.html#api)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DFX4Gl3wv7bi"
      },
      "source": [
        "\n",
        "#### How many images are in this collection?\n",
        "\n",
        "We can take our `images` variable try a couple of functions to get the same answer.\n",
        "\n",
        "1.   `len(images.index)`\n",
        "  * Get the length of the DataFrame's index.\n",
        "2.   `images.shape[0]`\n",
        "  * Get the shape or dimensionality of the DataFrame, and take the first item in the tuple.\n",
        "3.  `images.count()`\n",
        "  * Count the number of rows for each column.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HTv8Oet3jiTH"
      },
      "source": [
        "len(images.index)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6rYEERnTjifk"
      },
      "source": [
        "images.shape[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bn-1v127aKIG"
      },
      "source": [
        "images.count()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "38veKiPhwKo4"
      },
      "source": [
        " #### How many unique images are in the collection?\n",
        "\n",
        " We can see if an image is unique or not by computing an [MD5 hash](https://en.wikipedia.org/wiki/MD5#MD5_hashes) of it, and comparing them. The exact same image might have a filename of `example.jpg` or `foo.jpg`. If the hash is computed for each, we can see that even with different file names, they are actually the same image. So, since we have both a `MD5` and `SHA1` hash column available in our DataFrame, we can just find the unique values, and count them!\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WesM3kQowM5B"
      },
      "source": [
        "len(images.md5.unique())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZIXkI0-1wWQf"
      },
      "source": [
        "#### What are the top 10 most occurring images in the collection?\n",
        "\n",
        "Here we can take advantage of [`value_counts()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.value_counts.html) to provide us with a list of MD5 hashes, and their respective counts."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Ts03OFyjPIM"
      },
      "source": [
        "images[\"md5\"].value_counts().head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FG7pGZUEwlaI"
      },
      "source": [
        "\n",
        "#### What's the information around all of the occurances of `da5b449fff36752a93779fa4067cd2eb`?\n",
        "\n",
        "What, you mean you don't know what `da5b449fff36752a93779fa4067cd2eb` means? \n",
        "\n",
        "Let's find those images in the DataFrame. We can here see some of the filenames used, it's dimensions, and it's URL.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "msmmm65lkSIK"
      },
      "source": [
        "images.loc[images[\"md5\"] == \"da5b449fff36752a93779fa4067cd2eb\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kE-8epARIG0-"
      },
      "source": [
        "### What does `da5b449fff36752a93779fa4067cd2eb` look like?\n",
        "\n",
        "Let's grab the live web URL for the image, and then see if we can display it in a markdown cell.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wB3VqcmgJQM0"
      },
      "source": [
        "pd.options.display.max_colwidth = None\n",
        "one_image = images.loc[images[\"md5\"] == \"da5b449fff36752a93779fa4067cd2eb\"].head(1)\n",
        "one_image[\"url\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5YV2u_8aLEJS"
      },
      "source": [
        "![da5b449fff36752a93779fa4067cd2eb](https://t.teads.tv/track?action=placementCall&env=js-web&auctid=33f38483-62a4-4c26-b8a6-e1c7f33c1196&pageId=46587&pid=51781&debug_metadata=nYruSm6UJ5&fv=1107&ts=1670523222881&f=1&referer=https%3A%2F%2Fwww.milenio.com%2Fpolitica%2Fcomunidad%2F2020-fgjem-21-denuncias-violencia-genero-uaemex)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6YfsUGSRt1Ns"
      },
      "source": [
        "Unfortunately, we can't easily display it here since it is a tracker image!\n",
        "\n",
        "While we can see that this is the most popular image in the collection, we can't tell you _why_. That's where the researcher comes in!\n",
        "\n",
        "...though in this case, it's pretty obvious its for tracking/advertising since it's a 1x1 empty image.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DrDaY-C8iwhl"
      },
      "source": [
        "\n",
        "Another point of examination with the `images` DataFrame is the `height` and `width` columns. You could take a look at the largest images, or even `0x0` images, and potentially `spacer.gif` occurrences!\n",
        "\n",
        "* “[The invention and dissemination of the spacer gif: implications for the future of access and use of web archives](https://link.springer.com/article/10.1007/s42803-019-00006-8)”\n",
        "* \"[GeoCities and the spacer.gif](https://ruebot.net/post/geocities-and-the-spacer-gif/)\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GbLLZW2awzCv"
      },
      "source": [
        "#### What are the top 10 most occuring filenames in the collection?\n",
        "\n",
        "Note that this is of course different than the MD5 results up above. Here we are focusing _just_ on filename. So `cover.jpg` for example, might actually be referring to different images who happen to have the same name.\n",
        "\n",
        "Here we can use `value_counts()` again, but this time we'll create a variable for the top filenames so we can use it later.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pQaw54ACkwdZ"
      },
      "source": [
        "top_filenames = images[\"filename\"].value_counts().head(10)\n",
        "top_filenames"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z7F3re20BQRI"
      },
      "source": [
        "#### Let's create our first graph!\n",
        "\n",
        "We'll plot the data first with pandas [plot](https://pandas.pydata.org/docs/reference/api/pandas.Series.plot.html) functionality, and then plot the data with [Altair](https://altair-viz.github.io/)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sRvlstfsBWEZ"
      },
      "source": [
        "top_filenames_chart = top_filenames.plot.bar(figsize=(25, 10))\n",
        "\n",
        "top_filenames_chart.set_title(\"Top Filenames\", fontsize=22)\n",
        "top_filenames_chart.set_xlabel(\"Filename\", fontsize=20)\n",
        "top_filenames_chart.set_ylabel(\"Count\", fontsize=20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pQgeOObvgLvK"
      },
      "source": [
        "Now let's setup Altair, and plot the data with Altair. Altair is useful for creating vizualization since they can be easily exported as a PNG or SVG."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q7Z4J6qjWaVM"
      },
      "source": [
        "import altair as alt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s0xwvILYWkgg"
      },
      "source": [
        "top_filenames_altair = (\n",
        "    images[\"filename\"]\n",
        "    .value_counts()\n",
        "    .head(10)\n",
        "    .rename_axis(\"Filename\")\n",
        "    .reset_index(name=\"Count\")\n",
        ")\n",
        "\n",
        "filenames_bar = (\n",
        "    alt.Chart(top_filenames_altair)\n",
        "    .mark_bar()\n",
        "    .encode(x=alt.X(\"Filename:O\", sort=\"-y\"), y=alt.Y(\"Count:Q\"))\n",
        ")\n",
        "\n",
        "filenames_rule = (\n",
        "    alt.Chart(top_filenames_altair).mark_rule(color=\"red\").encode(y=\"mean(Count):Q\")\n",
        ")\n",
        "\n",
        "\n",
        "filenames_text = filenames_bar.mark_text(align=\"center\", baseline=\"bottom\").encode(\n",
        "    text=\"Count:Q\"\n",
        ")\n",
        "\n",
        "(filenames_bar + filenames_rule + filenames_text).properties(\n",
        "    width=1400, height=700, title=\"Top Filenames\"\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BneaN9cgGoly"
      },
      "source": [
        "#### How about a file format distribution?\n",
        "\n",
        "What _kind_ of image files are present? We can discover this by checking their \"media type\", or [MIME type](https://en.wikipedia.org/wiki/Media_type). \n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RDd-J8D-GwDk"
      },
      "source": [
        "image_mime_types = (\n",
        "    images[\"mime_type_tika\"]\n",
        "    .value_counts()\n",
        "    .head(5)\n",
        "    .rename_axis(\"MIME Type\")\n",
        "    .reset_index(name=\"Count\")\n",
        ")\n",
        "\n",
        "image_mimes_bar = (\n",
        "    alt.Chart(image_mime_types)\n",
        "    .mark_bar()\n",
        "    .encode(x=alt.X(\"MIME Type:O\", sort=\"-y\"), y=alt.Y(\"Count:Q\"))\n",
        ")\n",
        "\n",
        "image_mime_rule = (\n",
        "    alt.Chart(image_mime_types).mark_rule(color=\"red\").encode(y=\"mean(Count):Q\")\n",
        ")\n",
        "\n",
        "image_mime_text = image_mimes_bar.mark_text(align=\"center\", baseline=\"bottom\").encode(\n",
        "    text=\"Count:Q\"\n",
        ")\n",
        "\n",
        "(image_mimes_bar + image_mime_rule + image_mime_text).properties(\n",
        "    width=1400, height=700, title=\"Image File Format Distribution\"\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QUJR-jjqNxCL"
      },
      "source": [
        "#### How do I get the actual images?\n",
        "\n",
        "...or, how do I get to the actual binary files described by each file format information derivative?\n",
        "\n",
        "There are a few options!\n",
        "\n",
        "1. `wget` or `curl` from the live URL, or a replay URL\n",
        "  * Live web URL\n",
        "    * `wget` or `curl` the value of the `url` column\n",
        "  * Replay web URL\n",
        "    * `wget` or `curl` the value of the `crawl_date` and `url` column using the following pattern:\n",
        "      * `https://web.archive.org/web/` + `crawl_date` + `*/` + `url`\n",
        "        * https://web.archive.org/web/20120119*/http://www.archive.org/images/glogo.png\n",
        "2. Use a scripting language, such as Python\n",
        "  * Make use of the `url` and `filename` columns (and `crawl_date` if you want to use the replay URL)\n",
        "  * `import requests`\n",
        "  * `requests.get(url, allow_redirects=True)`\n",
        "  * `open('filename', 'wb').write(r.content)`\n",
        "3. Use the [Archives Unleashed Toolkit](https://aut.docs.archivesunleashed.org/docs/extract-binary) (if you have access to the W/ARC files)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dKqzuDVz-GiF"
      },
      "source": [
        "## Let's take a look at the domain frequency derivative."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QGHnjGRQPzqV"
      },
      "source": [
        "#### What does the distribution of domains look like?\n",
        "\n",
        "Here we can see which domains are the most frequent within the collection."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C_X_YSD4PyQi"
      },
      "source": [
        "top_domains = domain_frequency.sort_values(\"count\", ascending=False).head(10)\n",
        "\n",
        "top_domains_bar = (\n",
        "    alt.Chart(top_domains)\n",
        "    .mark_bar()\n",
        "    .encode(\n",
        "        x=alt.X(\"domain:O\", title=\"Domain\", sort=\"-y\"),\n",
        "        y=alt.Y(\"count:Q\", title=\"Count, Mean of Count\"),\n",
        "    )\n",
        ")\n",
        "\n",
        "top_domains_rule = (\n",
        "    alt.Chart(top_domains).mark_rule(color=\"red\").encode(y=\"mean(count):Q\")\n",
        ")\n",
        "\n",
        "top_domains_text = top_domains_bar.mark_text(align=\"center\", baseline=\"bottom\").encode(\n",
        "    text=\"count:Q\"\n",
        ")\n",
        "\n",
        "(top_domains_bar + top_domains_rule + top_domains_text).properties(\n",
        "    width=1400, height=700, title=\"Domains Distribution\"\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VDXDhqCcyyFj"
      },
      "source": [
        "### Top Level Domain Analysis\n",
        "\n",
        "pandas allows you to create new columns in a DataFrame based off of existing data. This comes in handy for a number of use cases with the available data that we have. In this case, let's create a new column, `tld`, which is based off an existing column, 'domain'. This example should provide you with an implementation pattern for expanding on these datasets to do further research and analysis.\n",
        "\n",
        "A [top-level domain](https://en.wikipedia.org/wiki/Top-level_domain) refers to the highest domain in an address - i.e. `.ca`, `.com`, `.org`, or yes, even `.pizza`.\n",
        "\n",
        "Things get a bit complicated, however, in some national TLDs. While `qc.ca` (the domain for Quebec) isn't really a top-level domain, it has many of the features of one as people can directly register under it. Below, we'll use the command `suffix` to include this. \n",
        "\n",
        "> You can learn more about suffixes at https://publicsuffix.org.\n",
        "\n",
        "We'll take the `domain` column and extract the `tld` from it with [`tldextract`](https://github.com/john-kurkowski/tldextract).\n",
        "\n",
        "First we'll add the [`tldextract`](https://github.com/john-kurkowski/tldextract) library to the notebook. Then, we'll create the new column."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "clPJuQAe5mcg"
      },
      "source": [
        "%%capture\n",
        "\n",
        "!pip install tldextract"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mv7a-MLIx-3f"
      },
      "source": [
        "import tldextract\n",
        "\n",
        "domain_frequency[\"tld\"] = domain_frequency.apply(\n",
        "    lambda row: tldextract.extract(row.domain).suffix, axis=1\n",
        ")\n",
        "domain_frequency"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jdXFS2yu8XYG"
      },
      "source": [
        "#### Next, let's count the distict TLDs.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1lViQIU48e-u"
      },
      "source": [
        "tld_count = domain_frequency[\"tld\"].value_counts()\n",
        "tld_count"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xm_V_0PGzZut"
      },
      "source": [
        "#### Next, we'll plot the TLD count.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N8yNlOa-zmBD"
      },
      "source": [
        "tld_count = (\n",
        "    domain_frequency[\"tld\"]\n",
        "    .value_counts()\n",
        "    .rename_axis(\"TLD\")\n",
        "    .reset_index(name=\"Count\")\n",
        "    .head(10)\n",
        ")\n",
        "\n",
        "tld_bar = (\n",
        "    alt.Chart(tld_count)\n",
        "    .mark_bar()\n",
        "    .encode(x=alt.X(\"TLD:O\", sort=\"-y\"), y=alt.Y(\"Count:Q\"))\n",
        ")\n",
        "\n",
        "tld_rule = alt.Chart(tld_count).mark_rule(color=\"red\").encode(y=\"mean(Count):Q\")\n",
        "\n",
        "tld_text = tld_bar.mark_text(align=\"center\", baseline=\"bottom\").encode(text=\"Count:Q\")\n",
        "\n",
        "(tld_bar + tld_rule + tld_text).properties(\n",
        "    width=1400, height=700, title=\"Top Level Domain Distribution\"\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sLriRk2UFlFP"
      },
      "source": [
        "## Web Crawl Frequency\n",
        "\n",
        "Let's see what the crawl frequency looks like by examining the `web_pages` DataFrame. First we'll create a new DataFrame by extracting the `crawl_date` and `domain` columns, and count the occurances of each domain and date combination."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_gGjFO6DAOYT"
      },
      "source": [
        "crawl_sites = web_pages[[\"crawl_date\", \"domain\"]]\n",
        "crawl_sites = crawl_sites.value_counts().reset_index()\n",
        "crawl_sites.columns = [\"Date\", \"Site\", \"Count\"]\n",
        "crawl_sites"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QCJLNokxGaBk"
      },
      "source": [
        "Next, we'll create a stacked bar chart where each bar will show the distribution of pages in that crawl by top-level domain.\n",
        "\n",
        "**NOTE**: Charts like this one work a lot better with collections that have more than 1 or 2 crawl dates. The temporal aspect is definitely something to take into consideration with each of the examples provided in this notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xQAx6JpNDL-I"
      },
      "source": [
        "## Altair has a default limit of 5000 rows, and this DataFrame is ~7700 rows, so we're going to disable the max allowed rows.\n",
        "alt.data_transformers.disable_max_rows()\n",
        "\n",
        "crawl_chart = (\n",
        "    alt.Chart(crawl_sites)\n",
        "    .mark_bar()\n",
        "    .encode(\n",
        "        x=\"Date:O\",\n",
        "        y=\"Count:Q\",\n",
        "        color=\"Site\",\n",
        "        tooltip=\"Site\",\n",
        "        order=alt.Order(\"Site\", sort=\"descending\"),\n",
        "    )\n",
        ")\n",
        "\n",
        "crawl_chart.properties(width=1400, height=700, title=\"Web Crawl Frequency\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q_PLHmrXZR34"
      },
      "source": [
        "## Examining the Web Graph\n",
        "\n",
        "Remember the hyperlink web graph? Let's look at the web graph columns again.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rBTHCPjWZlWV"
      },
      "source": [
        "web_graph"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K6j_2QrjZtd3"
      },
      "source": [
        "\n",
        "### What are the most frequent `source` and `target` combinations?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zOC5DePocC4c"
      },
      "source": [
        "top_links = web_graph[[\"source\", \"target\"]].value_counts().head(10).reset_index()\n",
        "top_links.columns = [\"source\", \"target\", \"count\"]\n",
        "top_links"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UceDp9wC4CWt"
      },
      "source": [
        "## Can we create a network graph visualization with the data we have?\n",
        "\n",
        "Yes! We can take advantage [NetworkX](https://networkx.org/documentation/stable/index.html) to create some basic graphs.\n",
        "\n",
        "NetworkX is *really* powerful, so there is a lot more that can be done with it than what we're demonstrating here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EPkV2asa4hXr"
      },
      "source": [
        "First we'll import `networkx` as well as `matplotlib.pyplot`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5axvp0L7OrhE"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import networkx as nx"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q_jSty3K4taF"
      },
      "source": [
        "We can take advantage of [`from_pandas_edgelist`](https://networkx.org/documentation/stable/reference/generated/networkx.convert_matrix.from_pandas_edgelist.html) here since our three graph derivatives are edge tables, and initialize our graph.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PoYrzuutOtru"
      },
      "source": [
        "G = nx.from_pandas_edgelist(\n",
        "    top_links, source=\"source\", target=\"target\", edge_key=\"target\", edge_attr=\"count\"\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0GXQAt9X5JIt"
      },
      "source": [
        "Setup our graph, and draw it!\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AkM5xZe83fJe"
      },
      "source": [
        "pos = nx.spring_layout(G, k=15)\n",
        "options = {\n",
        "    \"node_size\": 1000,\n",
        "    \"node_color\": \"#bc5090\",\n",
        "    \"node_shape\": \"o\",\n",
        "    \"alpha\": 0.5,\n",
        "    \"linewidths\": 4,\n",
        "    \"font_size\": 10,\n",
        "    \"font_color\": \"black\",\n",
        "    \"width\": 2,\n",
        "    \"edge_color\": \"grey\",\n",
        "}\n",
        "\n",
        "plt.figure(figsize=(12, 12))\n",
        "\n",
        "nx.draw(G, pos, with_labels=True, **options)\n",
        "\n",
        "labels = {e: G.edges[e][\"count\"] for e in G.edges}\n",
        "nx.draw_networkx_edge_labels(G, pos, edge_labels=labels)\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wTcTLMHmTGXo"
      },
      "source": [
        "## Text Analysis\n",
        "\n",
        "Next, we'll do some basic text analysis with our `web_pages` DataFrame with `nltk` and`spaCy`, and end with a word cloud.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dPzVa3WhTImQ"
      },
      "source": [
        "import re\n",
        "\n",
        "import nltk"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1QkkITU2TkkB"
      },
      "source": [
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"stopwords\")\n",
        "nltk.download(\"wordnet\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9x1k3fmHu1NU"
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mvmymu758duP"
      },
      "source": [
        "We'll drop the `NaN` values in our DataFrame to clean things up a bit."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nBEVNyMxjtHg"
      },
      "source": [
        "web_pages = web_pages.dropna()\n",
        "web_pages"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dArfB41Vp5Rx"
      },
      "source": [
        "We need to set the [`mode.chained_assignment`](https://pandas.pydata.org/docs/user_guide/options.html?highlight=chained_assignment) to `None` now to silence some exception errors that will come up."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UelY00RWxs_-"
      },
      "source": [
        "pd.options.mode.chained_assignment = None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GCg-yGKl8ne8"
      },
      "source": [
        "Next, we'll setup a tokenizer which will split on words, and create a new column which is the tokenized text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vwtFa9FfzLjC"
      },
      "source": [
        "tokenizer = nltk.RegexpTokenizer(r\"\\w+\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SGSTGsdHr0l3"
      },
      "source": [
        "web_pages[\"content_tokenized\"] = web_pages[\"content\"].map(tokenizer.tokenize)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-e4qcA8p85Od"
      },
      "source": [
        "Now well create a column with the tokenized value count."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XOXKqykU0Vgc"
      },
      "source": [
        "web_pages[\"content_tokens\"] = web_pages[\"content_tokenized\"].apply(lambda x: len(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o1LvlTd-9GC9"
      },
      "source": [
        "### Basic word count statistics with pandas!\n",
        "\n",
        "Now we can use the power of pandas [Statisitcal functions](https://pandas.pydata.org/docs/user_guide/computation.html) to show us some basic statistics about the tokens."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8wvUcbx29M4n"
      },
      "source": [
        "**Mean**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cwO0KHnS9JJQ"
      },
      "source": [
        "web_pages[\"content_tokens\"].mean()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "idMyBDrU9V3O"
      },
      "source": [
        "**Standard deviation**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G-V6zi079WNv"
      },
      "source": [
        "web_pages[\"content_tokens\"].std()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ai0QVx9t9bnt"
      },
      "source": [
        "**Max**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qbXjWwW99bzr"
      },
      "source": [
        "web_pages[\"content_tokens\"].max()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bLT5HLXM9f8W"
      },
      "source": [
        "**Min**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "629g31k39gH2"
      },
      "source": [
        "web_pages[\"content_tokens\"].min()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b-bIw1Pu2NkW"
      },
      "source": [
        "### Pages with most words\n",
        "\n",
        "Let's create a bar chart that shows the pages with the most words. Here we can see the power of pandas at work, in terms of both analysis and visualization.\n",
        "\n",
        "First, let's show the query to get the data for our chart."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oktgvxIr2cSV"
      },
      "source": [
        "word_count = (\n",
        "    web_pages[[\"url\", \"content_tokens\"]]\n",
        "    .sort_values(by=\"content_tokens\", ascending=False)\n",
        "    .head(25)\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Eeserah2lBX"
      },
      "source": [
        "word_count"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uepwHXS59tz-"
      },
      "source": [
        "Next, let's create a bar chart of this."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ns1ZlwbnqoDi"
      },
      "source": [
        "word_count_bar = (\n",
        "    alt.Chart(word_count)\n",
        "    .mark_bar()\n",
        "    .encode(x=alt.X(\"url:O\", sort=\"-y\"), y=alt.Y(\"content_tokens:Q\"))\n",
        ")\n",
        "\n",
        "word_count_rule = (\n",
        "    alt.Chart(word_count).mark_rule(color=\"red\").encode(y=\"mean(content_tokens):Q\")\n",
        ")\n",
        "\n",
        "word_count_text = word_count_bar.mark_text(align=\"center\", baseline=\"bottom\").encode(\n",
        "    text=\"content_tokens:Q\"\n",
        ")\n",
        "\n",
        "(word_count_bar + word_count_rule + word_count_text).properties(\n",
        "    width=1400, height=700, title=\"Pages with the most words\"\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uhLIldv-3SOm"
      },
      "source": [
        "### How about NER on the page with the most tokens?\n",
        "\n",
        "[Named-Entity Recognition](https://en.wikipedia.org/wiki/Named-entity_recognition), or NER, is an exciting field of natural language processing that lets us extract \"entities\" out of text; the names of people, locations, or organizations.\n",
        "\n",
        "To do this, we first need to find the pages that have the most tokens."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Svg3ruY3cQg"
      },
      "source": [
        "word_count_max = (\n",
        "    web_pages[[\"url\", \"content_tokens\", \"content\"]]\n",
        "    .sort_values(by=\"content_tokens\", ascending=False)\n",
        "    .head(1)\n",
        ")\n",
        "word_count_max[\"url\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DvufosIP-7JH"
      },
      "source": [
        "We'll remove the column width limit so we can check out our content for the page."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_gLaHabY-vRM"
      },
      "source": [
        "pd.set_option(\"display.max_colwidth\", None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sBxWH3MV_Gp-"
      },
      "source": [
        "Let's take a look at our page's content."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5WykQM1H3YIU"
      },
      "source": [
        "page = word_count_max[\"content\"].astype(\"unicode\").to_string()\n",
        "page"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zVEwDKpR5QcB"
      },
      "source": [
        "\n",
        "#### Setup spaCy\n",
        "\n",
        "We now need to set up [spaCy](https://en.wikipedia.org/wiki/SpaCy), a natural-language processing toolkit, and we will use the Spanish language package since the collection we are working with is in Spanish!\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!python -m spacy download es_core_news_sm"
      ],
      "metadata": {
        "id": "97cwoYYVW4iY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7D8tBLZz3wBp"
      },
      "source": [
        "import spacy\n",
        "from spacy import displacy\n",
        "\n",
        "nlp = spacy.load(\"es_core_news_sm\")\n",
        "\n",
        "nlp.max_length = 1100000"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pIPHPYyIEEwN"
      },
      "source": [
        "Next we'll run the natual language processor from SpaCy, and then display the NER output. Watch how it finds organizations, people, and beyond!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7LNVyHX633qT"
      },
      "source": [
        "ner = nlp(page)\n",
        "displacy.render(ner, style=\"ent\", jupyter=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t-KLFb415e1y"
      },
      "source": [
        "### Wordcloud\n",
        "\n",
        "What better way to wrap-up this notebook than create a word cloud!\n",
        "\n",
        "Word clouds are always fun, right?! They're an interesting way to visualize word frequency, as the more times that a word occurs, the larger it will appear in the word cloud.\n",
        "\n",
        "Let's setup some dependencies here. We will install the [word_cloud](https://github.com/amueller/word_cloud) library, and setup some stop words via `nltk`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qiS7nhq2eirZ"
      },
      "source": [
        "%%capture\n",
        "\n",
        "!pip install wordcloud\n",
        "from wordcloud import WordCloud, ImageColorGenerator"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZvKSPMvE95m_"
      },
      "source": [
        "Let's remove the remove the stopwords from our data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0E06S_xO5-7c"
      },
      "source": [
        "stopwords = stopwords.words(\"spanish\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pbcXX78s6pFH"
      },
      "source": [
        "web_pages[\"stopwords\"] = web_pages[\"content_tokenized\"].apply(\n",
        "    lambda x: [item.lower() for item in x if item not in stopwords]\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j6IrEAcTEorT"
      },
      "source": [
        "Next we'll pull 500 rows of values from our new column."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l-DROr3veo3J"
      },
      "source": [
        "words = web_pages[\"stopwords\"].head(500)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qgiOIFUU9_3I"
      },
      "source": [
        "Now we can create a word cloud!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Kp0xnU1eud6"
      },
      "source": [
        "wordcloud = WordCloud(\n",
        "    width=2000,\n",
        "    height=1500,\n",
        "    scale=10,\n",
        "    max_font_size=250,\n",
        "    max_words=100,\n",
        "    background_color=\"white\",\n",
        ").generate(str(words))\n",
        "plt.figure(figsize=[35, 10])\n",
        "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}